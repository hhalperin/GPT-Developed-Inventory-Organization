# -*- coding: utf-8 -*-
"""GPT Developed Inventory Organization

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mrT-tymKAo80GPjpjjjJeYnOlsnSZN7j

## API KEY
"""

# Set the environment variable
from google.colab import userdata
api_key = userdata.get('API_KEY')
print(api_key)

"""Running the Warehouse Inventory Category Classifier to Gather Data
---
---
"""

import os
import time
import logging
import warnings
import requests
import pandas as pd
import numpy as np
import re
from typing import Dict, Tuple, Any, List
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from tqdm.auto import tqdm
from rapidfuzz import fuzz  # Using RapidFuzz for fuzzy string matching
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import LabelEncoder
from sklearn.utils.validation import check_is_fitted
from sklearn.decomposition import TruncatedSVD
from sklearn.cluster import KMeans
import joblib

# Configure logging and warnings
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s"
)
warnings.filterwarnings("ignore", category=FutureWarning)

# Global configuration for API and rate limiting
MODEL = "gpt-4.1-nano-2025-04-14"
MAX_TOKENS = 35
TOP_P = 0.05
TEMPERATURE = 0.1
API_URL = "https://api.openai.com/v1/chat/completions"

# Mapping of manufacturer codes to full manufacturer names
MFR_DICT: Dict[str, str] = {
    "DOT": "Dottie",
    "CH": "Eaton",
    "BLINE": "Cooper B-Line",
    "MIL": "Milbank",
    "LEV": "Leviton",
    "ITE": "Siemens",
    "GEIND": "General Electric Industrial",
    "UNIPA": "Union Pacific",
    "GARV": "Garvin Industries",
    "FIT": "American Fittings",
    "TAY": "TayMac",
    "ARL": "Arlington",
    "AMFI": "American Fittings",
    "BPT": "Bridgeport",
    "CCHO": "Eaton Course-Hinds",
    "HARGR": "Harger",
    "CARLN": "Carlon",
    "MULB": "Mulberry",
    "SOLAR": "Solarline",
    "ENERL": "Enerlites",
    "HUBWD": "Hubble Wiring Device",
    "DMC": "DMC Power",
    "INT": "Intermatic",
    "LUT": "Lutron",
    "LITTE": "Littelfuse",
    "GRNGA": "GreenGate",
    "WATT": "Wattstopper",
    "SENSO": "Sensor Switch",
    "CHE": "Eaton Crouse Hinds",
    "OZ": "OZ Gedney",
}

def clean_data_frame(df: pd.DataFrame) -> pd.DataFrame:
    """
    Cleans the DataFrame by removing empty rows and expanding manufacturer codes.

    Args:
        df: Original DataFrame with bin location data.

    Returns:
        Cleaned DataFrame.
    """
    cleaned_df = df[df["CatalogNo"] != "EMPTY"].copy()
    cleaned_df.loc[:, "MfrCode"] = cleaned_df["MfrCode"].apply(lambda x: MFR_DICT.get(x, x))
    return cleaned_df

def get_bin_data(df: pd.DataFrame) -> Dict[Any, Any]:
    """
    Converts the cleaned DataFrame to a dictionary.

    Args:
        df: Cleaned DataFrame.

    Returns:
        Dictionary representation of the DataFrame.
    """
    return df.to_dict(orient="index")

class WarehouseInventory:
    """
    Manages warehouse inventory data, enriches part descriptions via GPT,
    computes similarity scores (using fuzzy matching for SKUs), and categorizes parts.
    """

    def __init__(self, data: pd.DataFrame) -> None:
        self.data: pd.DataFrame = data.copy()
        self.data["Main Category"] = None
        self.data["Sub-category"] = None
        self.sku_similarity_dict: Dict[str, Dict[str, float]] = {}
        self.description_similarity_dict: Dict[str, Dict[str, float]] = {}
        self.filtered_sku_similarity_dict: Dict[str, Dict[str, float]] = {}
        self.filtered_description_similarity_dict: Dict[str, Dict[str, float]] = {}
        self.existing_categories: Dict[str, List[Dict[str, Any]]] = {}
        self.category_change_dict: Dict[str, Dict[str, Any]] = {}
        self.timing_info: Dict[str, float] = {}
        self.ml_model = MLCategorizer()

        self.categorization_stats: Dict[str, float] = {}
        self.data["Highest SKU Similarity Score"] = None
        self.data["Most Similar SKU"] = None
        self.data["Highest Description Similarity Score"] = None
        self.data["Most Similar Description SKU"] = None

    @staticmethod
    def call_gpt_api(prompt: str) -> str:
        """
        Calls the OpenAI GPT API with a given prompt and returns the response text.

        Args:
            prompt: The prompt to send to the GPT API.

        Returns:
            The response from the GPT API.
        """
        time.sleep(0.01)  # Simple rate limiting adjustment
        headers = {"Authorization": f"Bearer {api_key}"}
        payload = {
            "model": MODEL,
            "messages": [
                {
                    "role": "system",
                    "content": ("You are a professional electrician and expert in electrical supply parts. "
                                "Provide short, plain-English responses.")
                },
                {"role": "user", "content": prompt}
            ],
            "max_tokens": MAX_TOKENS,
            "top_p": TOP_P,
            "temperature": TEMPERATURE
        }
        try:
            response = requests.post(API_URL, headers=headers, json=payload)
            response.raise_for_status()
            result = response.json()
            return result["choices"][0]["message"]["content"].strip()
        except requests.RequestException as e:
            logging.error(f"API call error: {e}")
            return ""

    def enrich_description(self, catalog_no: str, description: str, mfr_code: str) -> str:
        """
        Uses GPT to produce an enriched, short description for a part.

        Args:
            catalog_no: The catalog number for the part.
            description: The abbreviated part description.
            mfr_code: The manufacturer code.

        Returns:
            Enriched description.
        """
        prompt = (f"Create an enriched part description in plain English (10 words or less). "
                  f"CatalogNo: {catalog_no}, Manufacturer: {mfr_code}, Abbreviated Description: {description}.")
        return self.call_gpt_api(prompt)

    def enrich_all_descriptions(self) -> None:
        """Enrich all part descriptions in the data using the GPT API."""
        for index, row in tqdm(self.data.iterrows(), total=len(self.data), desc="Enriching Descriptions"):
            catalog_no = row["CatalogNo"]
            description = row["Description"]
            mfr_code = row["MfrCode"]
            enriched = self.enrich_description(catalog_no, description, mfr_code)
            self.data.at[index, "Enriched Description"] = enriched

    def compute_similarity_scores(self) -> None:
        """
        Compute and store similarity scores for SKUs using fuzzy matching
        (RapidFuzz) and for enriched descriptions using TF-IDF.
        """
        catalog_nos = self.data["CatalogNo"].fillna("").tolist()
        # Compute SKU similarity using RapidFuzz (normalized to 0-1)
        for sku in catalog_nos:
            self.sku_similarity_dict[sku] = {}
        for i, sku in enumerate(catalog_nos):
            for j, other_sku in enumerate(catalog_nos):
                score = fuzz.ratio(sku, other_sku) / 100.0
                self.sku_similarity_dict[sku][other_sku] = score

        # Compute description similarity with TF-IDF
        descriptions = self.data["Enriched Description"].fillna("").tolist()
        desc_vectorizer = TfidfVectorizer()
        desc_vectors = desc_vectorizer.fit_transform(descriptions)
        desc_sim_matrix = cosine_similarity(desc_vectors)
        for i, sku in enumerate(catalog_nos):
            self.description_similarity_dict[sku] = {}
            for j, other_sku in enumerate(catalog_nos):
                self.description_similarity_dict[sku][other_sku] = desc_sim_matrix[i, j]

        # Apply a default threshold to filter similarities
        default_threshold = 0.8
        for sku, sim in self.sku_similarity_dict.items():
            self.filtered_sku_similarity_dict[sku] = {k: v for k, v in sim.items() if v >= default_threshold}
        for sku, sim in self.description_similarity_dict.items():
            self.filtered_description_similarity_dict[sku] = {k: v for k, v in sim.items() if v >= default_threshold}

    def update_dataframe_with_similarity(self) -> None:
        """
        Update the DataFrame with the highest similarity scores for both SKU and description.
        """
        highest_sku_sim: Dict[str, Dict[str, Any]] = {}
        highest_desc_sim: Dict[str, Dict[str, Any]] = {}
        for sku, sim_dict in self.filtered_sku_similarity_dict.items():
            filtered = {k: v for k, v in sim_dict.items() if k != sku}
            if filtered:
                best_match = max(filtered, key=filtered.get)
                highest_sku_sim[sku] = {"Highest SKU Similarity Score": filtered[best_match],
                                          "Most Similar SKU": best_match}
        for sku, sim_dict in self.filtered_description_similarity_dict.items():
            filtered = {k: v for k, v in sim_dict.items() if k != sku}
            if filtered:
                best_match = max(filtered, key=filtered.get)
                highest_desc_sim[sku] = {"Highest Description Similarity Score": filtered[best_match],
                                           "Most Similar Description SKU": best_match}
        self.data["Highest SKU Similarity Score"] = self.data["CatalogNo"].map(
            lambda sku: highest_sku_sim.get(sku, {}).get("Highest SKU Similarity Score")
        )
        self.data["Most Similar SKU"] = self.data["CatalogNo"].map(
            lambda sku: highest_sku_sim.get(sku, {}).get("Most Similar SKU")
        )
        self.data["Highest Description Similarity Score"] = self.data["CatalogNo"].map(
            lambda sku: highest_desc_sim.get(sku, {}).get("Highest Description Similarity Score")
        )
        self.data["Most Similar Description SKU"] = self.data["CatalogNo"].map(
            lambda sku: highest_desc_sim.get(sku, {}).get("Most Similar Description SKU")
        )

    def assign_category(self, catalog_no: str, mfr_code: str) -> Tuple[str, str]:
        enriched_desc = self.data.loc[self.data["CatalogNo"] == catalog_no, "Enriched Description"].values[0]
        item = {
            "CatalogNo": catalog_no,
            "Enriched Description": enriched_desc
        }

        try:
            ml_main, ml_sub, confidence = self.ml_model.predict(item)
            if confidence >= 0.75:
                self.update_existing_categories(ml_main, ml_sub, catalog_no)
                return ml_main, ml_sub
        except Exception as e:
            logging.warning(f"ML prediction failed for {catalog_no}: {e}")

        # fallback to GPT if ML fails or confidence too low
        main_category = self.determine_category_gpt(catalog_no, mfr_code, enriched_desc, "main")
        sub_category = self.determine_category_gpt(catalog_no, mfr_code, enriched_desc, "sub", main_category)
        self.update_existing_categories(main_category, sub_category, catalog_no)
        return main_category, sub_category


    def determine_category_gpt(self, catalog_no: str, mfr_code: str, enriched_desc: str, category_type: str, main_category: str = "") -> str:
        """
        Determines a category using GPT with a dynamic prompt.
        """
        if category_type == "main":
            prompt = (f"Determine the main category of {catalog_no} using description: '{enriched_desc}' "
                      f"and manufacturer: {mfr_code}. Format output as: 'Main Category: (main_category)'")
        else:
            prompt = (f"Determine the sub-category of {catalog_no} more specific than {main_category} using description: "
                      f"'{enriched_desc}' and manufacturer: {mfr_code}. Format output as: 'Sub Category: (sub_category)'")
        response = self.call_gpt_api(prompt)
        return response.capitalize()

    def update_existing_categories(self, main_category: str, sub_category: str, catalog_no: str) -> None:
        """
        Updates the internal mapping of categories.
        """
        new_row = {"CatalogNo": catalog_no, "Main Category": main_category, "Sub-category": sub_category}
        if main_category not in self.existing_categories:
            self.existing_categories[main_category] = []
        self.existing_categories[main_category].append(new_row)

    def reevaluate_categories(self) -> None:
        """
        Recomputes categories using GPT and updates statistics if differences are found.
        """
        self.category_change_dict = {}
        start_time = time.time()
        for index, row in tqdm(self.data.iterrows(), total=len(self.data), desc="Reevaluating Categories"):
            catalog_no = row["CatalogNo"]
            old_main = row.get("Main Category", None)
            old_sub = row.get("Sub-category", None)
            enriched_desc = row.get("Enriched Description", "")
            new_main = self.determine_category_gpt(catalog_no, row["MfrCode"], enriched_desc, "main")
            new_sub = self.determine_category_gpt(catalog_no, row["MfrCode"], enriched_desc, "sub", new_main)
            if old_main != new_main or old_sub != new_sub:
                self.category_change_dict[catalog_no] = {
                    "Old Main": old_main, "New Main": new_main,
                    "Old Sub": old_sub, "New Sub": new_sub
                }
                self.data.at[index, "Main Category"] = new_main
                self.data.at[index, "Sub-category"] = new_sub
        self.timing_info["reevaluate_categories"] = time.time() - start_time
        total_items = len(self.data)
        changed_main = len([c for c in self.category_change_dict.values() if c["Old Main"] != c["New Main"]])
        changed_sub = len([c for c in self.category_change_dict.values() if c["Old Sub"] != c["New Sub"]])
        self.categorization_stats = {
            "Main Category Changed %": (changed_main / total_items) * 100,
            "Sub Category Changed %": (changed_sub / total_items) * 100,
        }

    def apply_threshold_filter(self, threshold: float) -> None:
        """
        Filters similarity dictionaries based on the provided threshold.
        """
        self.filtered_sku_similarity_dict = {
            key: {k: v for k, v in value.items() if v >= threshold}
            for key, value in self.sku_similarity_dict.items()
        }
        self.filtered_description_similarity_dict = {
            key: {k: v for k, v in value.items() if v >= threshold}
            for key, value in self.description_similarity_dict.items()
        }

    def compute_average_scores(self) -> None:
        """
        Computes the average similarity scores for each part.
        """
        for catalog_no, sims in self.sku_similarity_dict.items():
            all_scores = list(sims.values())
            high_scores = [score for score in all_scores if score > 0.5]
            avg_all = sum(all_scores) / len(all_scores) if all_scores else 0
            avg_high = sum(high_scores) / len(high_scores) if high_scores else 0
            self.data.loc[self.data["CatalogNo"] == catalog_no, "Average Similarity Score"] = avg_all
            self.data.loc[self.data["CatalogNo"] == catalog_no, "Average High Similarity Score"] = avg_high

    def test_on_subset(self, fraction: float = 0.1, thresholds: List[float] = [0.75, 0.8, 0.85, 0.9]) -> List[Dict[str, Any]]:
        """
        Runs tests on a subset of the data for varying threshold values and saves results.

        Args:
            fraction: Fraction of the data to sample.
            thresholds: A list of similarity thresholds to test.

        Returns:
            A list of dictionaries containing test results and statistics for each threshold.
        """
        sample_data = self.data.sample(frac=fraction)
        temp_inventory = WarehouseInventory(sample_data)



        # Only enrich if column doesn't exist or is mostly missing
        if "Enriched Description" not in temp_inventory.data.columns or \
        temp_inventory.data["Enriched Description"].isnull().mean() > 0.2:
            logging.info("Enriching descriptions for test subset.")
            temp_inventory.enrich_all_descriptions()
        else:
            logging.info("Enriched descriptions already present in test subset.")

        results_dir = "threshold_test_results"
        os.makedirs(results_dir, exist_ok=True)

        # Determine which thresholds still need running
        existing = set(os.listdir(results_dir))
        to_run = [
            t for t in thresholds
            if f"results_threshold_{t}.xlsx" not in existing
        ]

        results = []
        for threshold in thresholds:
            temp_inventory.apply_threshold_filter(threshold)
            temp_inventory.compute_similarity_scores()
            temp_inventory.update_dataframe_with_similarity()
            temp_inventory.reevaluate_categories()
            temp_inventory.compute_average_scores()
            filename = f"{results_dir}/results_threshold_{threshold}.xlsx"
            temp_inventory.data.to_excel(filename, index=False)
            logging.info(f"Results for threshold {threshold} saved to {filename}")
            results.append({
                "threshold": threshold,
                "data": temp_inventory.data.copy(),
                "stats": temp_inventory.categorization_stats,
                "timing_info": temp_inventory.timing_info
            })

        for result in results:
            logging.info(f"Threshold {result['threshold']}: Timing {result['timing_info']}, Stats {result['stats']}")
        return results

    def show_dataframe(self) -> pd.DataFrame:
        print(f"DataFrame shape: {self.data.shape}")
        print(f"DataFrame columns: {self.data.columns}")
        print(f"DataFrame head:\n{self.data.head()}")

        return self.data.copy()

class MLCategorizer:
    def __init__(self):
        self.main_category_model = RandomForestClassifier(n_estimators=100)
        self.sub_category_models = {}
        self.tfidf = TfidfVectorizer(max_features=1000)
        self.svd = None  # Will be fitted dynamically
        self.main_label_encoder = LabelEncoder()
        self.sub_label_encoders = {}
        self.cluster_model = None

    def _extract_sku_features(self, sku):
        return np.array([
            len(sku),
            sum(c.isdigit() for c in sku),
            sum(c.isalpha() for c in sku)
        ])

    def _compute_svd(self, tfidf_matrix):
        n_features = tfidf_matrix.shape[1]
        n_components = min(100, n_features - 1) if n_features > 1 else 1
        self.svd = TruncatedSVD(n_components=n_components)
        logging.info(f"Using {n_components} SVD components out of {n_features} TF-IDF features.")
        return self.svd.fit_transform(tfidf_matrix)

    def prepare_features(self, data, fit=True):
        descs = data['Enriched Description'].fillna("")

        if fit:
            tfidf_matrix = self.tfidf.fit_transform(descs)
            reduced_desc = self._compute_svd(tfidf_matrix)
        else:
            check_is_fitted(self.tfidf, "vocabulary_")
            if self.svd is None:
                raise RuntimeError("SVD transformer is not fitted. Call `.train()` first.")
            tfidf_matrix = self.tfidf.transform(descs)
            reduced_desc = self.svd.transform(tfidf_matrix)

        sku_features = np.array([self._extract_sku_features(sku) for sku in data['CatalogNo']])
        return np.hstack([reduced_desc, sku_features])

    def train(self, data):
        X = self.prepare_features(data, fit=True)
        y_main = self.main_label_encoder.fit_transform(data['Main Category'])
        self.main_category_model.fit(X, y_main)

        for main_cat in data['Main Category'].unique():
            mask = data['Main Category'] == main_cat
            if sum(mask) > 10:
                sub_data = data[mask]
                X_sub = self.prepare_features(sub_data, fit=False)
                y_sub = LabelEncoder()
                y_enc = y_sub.fit_transform(sub_data['Sub-category'])
                clf = RandomForestClassifier(n_estimators=100)
                clf.fit(X_sub, y_enc)
                self.sub_label_encoders[main_cat] = y_sub
                self.sub_category_models[main_cat] = clf

    def predict(self, item):
        item_df = pd.DataFrame([item])
        X = self.prepare_features(item_df, fit=False)
        main_cat_proba = self.main_category_model.predict_proba(X)[0]
        main_idx = np.argmax(main_cat_proba)
        main_cat = self.main_label_encoder.inverse_transform([main_idx])[0]

        sub_cat = None
        if main_cat in self.sub_category_models:
            sub_clf = self.sub_category_models[main_cat]
            sub_proba = sub_clf.predict_proba(X)[0]
            sub_idx = np.argmax(sub_proba)
            sub_encoder = self.sub_label_encoders[main_cat]
            sub_cat = sub_encoder.inverse_transform([sub_idx])[0]

        return main_cat, sub_cat, max(main_cat_proba)

    def fit_kmeans(self, descriptions, n_clusters=20):
        desc_matrix = self.tfidf.fit_transform(descriptions)
        self.cluster_model = KMeans(n_clusters=n_clusters)
        self.cluster_model.fit(desc_matrix)

    def assign_clusters(self, descriptions):
        if self.cluster_model:
            desc_matrix = self.tfidf.transform(descriptions)
            return self.cluster_model.predict(desc_matrix)
        else:
            return [-1] * len(descriptions)

    def save_models(self, prefix='model'):
        joblib.dump(self.main_category_model, f'{prefix}_main.pkl')
        joblib.dump(self.main_label_encoder, f'{prefix}_main_encoder.pkl')
        for main_cat, model in self.sub_category_models.items():
            joblib.dump(model, f'{prefix}_sub_{main_cat}.pkl')
            joblib.dump(self.sub_label_encoders[main_cat], f'{prefix}_sub_encoder_{main_cat}.pkl')
        joblib.dump(self.tfidf, f'{prefix}_tfidf.pkl')
        joblib.dump(self.svd, f'{prefix}_svd.pkl')
        if self.cluster_model:
            joblib.dump(self.cluster_model, f'{prefix}_kmeans.pkl')

if __name__ == "__main__":
    try:
        # STEP 1 — Load categorized or enriched data
        if os.path.exists("categorized_data.xlsx"):
            df = pd.read_excel("categorized_data.xlsx")
            logging.info("Loaded categorized_data.xlsx successfully.")
        elif os.path.exists("enriched_data.xlsx"):
            df = pd.read_excel("enriched_data.xlsx")
            logging.info("Loaded enriched_data.xlsx successfully.")
        else:
            raw_data = pd.read_excel("inventory_data.xlsx")
            logging.info("Loaded raw Excel file successfully.")
            cleaned_data = clean_data_frame(raw_data)
            warehouse_inventory = WarehouseInventory(cleaned_data)
            warehouse_inventory.enrich_all_descriptions()
            df = warehouse_inventory.data
            df.to_excel("enriched_data.xlsx", index=False)
            logging.info("Saved enriched_data.xlsx.")

        # STEP 2 — Initialize inventory object
        warehouse_inventory = WarehouseInventory(df)

        # STEP 3 — Train model first so it’s ready for fallback logic
        ml_model = MLCategorizer()
        ml_model.train(warehouse_inventory.data)
        warehouse_inventory.ml_model = ml_model

        # STEP 4 — Assign categories if not present or incomplete
        if "Main Category" not in df.columns or "Sub-category" not in df.columns or \
           warehouse_inventory.data["Main Category"].isna().any() or warehouse_inventory.data["Sub-category"].isna().any():
            logging.info("Running fallback category assignment (ML + GPT)...")
            for index, row in warehouse_inventory.data.iterrows():
                catalog_no = row["CatalogNo"]
                mfr_code = row["MfrCode"]
                try:
                    main_cat, sub_cat = warehouse_inventory.assign_category(catalog_no, mfr_code)
                except Exception as e:
                    logging.warning(f"Category fallback failed for {catalog_no}: {e}")
                    main_cat, sub_cat = "Uncategorized", "Uncategorized"
                warehouse_inventory.data.at[index, "Main Category"] = main_cat
                warehouse_inventory.data.at[index, "Sub-category"] = sub_cat

            # ✅ SAVE categorized results
            warehouse_inventory.data.to_excel("categorized_data.xlsx", index=False)
            logging.info("Saved categorized data to categorized_data.xlsx.")

        else:
            logging.info("Category columns already populated — skipping categorization.")

        # STEP 5 — Continue with rest of pipeline
        warehouse_inventory.compute_similarity_scores()
        warehouse_inventory.update_dataframe_with_similarity()
        warehouse_inventory.reevaluate_categories()
        warehouse_inventory.compute_average_scores()
        test_results = warehouse_inventory.test_on_subset(fraction=1.0)

        # STEP 6 — Output preview
        logging.info("Pipeline completed. Here's a preview:")
        print(warehouse_inventory.show_dataframe())

    except Exception as e:
        logging.error(f"Fatal error in pipeline: {e}")

import os
import glob
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import DBSCAN, KMeans
from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score
from sklearn.neighbors import NearestNeighbors
import networkx as nx
import warnings
warnings.filterwarnings("ignore")

sns.set_style("whitegrid")

# Part 1: DBSCAN Threshold-Based Clustering
threshold_dir = '/content/threshold_test_results/'
files = glob.glob(os.path.join(threshold_dir, 'results_threshold_*.xlsx'))
file_thresholds = []
for file in files:
    name = os.path.basename(file)
    try:
        threshold_str = name.split('_')[2].replace('.xlsx', '')
        threshold_val = float(threshold_str)
    except:
        continue
    file_thresholds.append((threshold_val, file))
file_thresholds.sort(key=lambda x: x[0])

# Collect all descriptions from all threshold files
all_descs = []
file_ranges = []
current_index = 0
for threshold, file in file_thresholds:
    df = pd.read_excel(file)
    if 'Enriched Description' not in df.columns:
        continue
    descs = df['Enriched Description'].dropna().astype(str)
    descs = descs[descs.str.strip() != '']
    n = len(descs)
    if n == 0:
        continue
    file_ranges.append((threshold, current_index, current_index + n))
    current_index += n
    all_descs.extend(descs.tolist())

if len(all_descs) > 0:
    vectorizer = TfidfVectorizer()
    X_all = vectorizer.fit_transform(all_descs)
else:
    X_all = None

dbscan_metrics = []
for threshold, start_idx, end_idx in tqdm(file_ranges, desc="DBSCAN thresholds"):
    X = X_all[start_idx:end_idx]
    if X.shape[0] < 2:
        continue
    eps = 1.0 - threshold
    db = DBSCAN(eps=eps, metric='cosine', min_samples=2)
    labels = db.fit_predict(X)
    mask = labels != -1
    labels_valid = labels[mask]
    X_valid = X[mask]
    if len(labels_valid) < 2 or len(set(labels_valid)) < 2:
        sil_score = np.nan
        ch_score = np.nan
        db_score = np.nan
    else:
        X_valid_arr = X_valid.toarray()
        sil_score = silhouette_score(X_valid_arr, labels_valid, metric='cosine')
        ch_score = calinski_harabasz_score(X_valid_arr, labels_valid)
        db_score = davies_bouldin_score(X_valid_arr, labels_valid)
    n_samples = X.shape[0]
    if n_samples > 0:
        neigh = NearestNeighbors(radius=eps, metric='cosine').fit(X)
        distances, indices = neigh.radius_neighbors(X)
        G = nx.Graph()
        G.add_nodes_from(range(n_samples))
        for i, neighbors in enumerate(indices):
            for j in neighbors:
                if i < j:
                    G.add_edge(i, j)
        degrees = [deg for _, deg in G.degree()]
        avg_degree = np.mean(degrees) if n_samples > 0 else np.nan
        components = list(nx.connected_components(G))
        num_components = len(components)
        largest_comp = max((len(comp) for comp in components), default=0)
    else:
        avg_degree = np.nan
        num_components = np.nan
        largest_comp = np.nan
    dbscan_metrics.append({
        'threshold': threshold,
        'silhouette_score': sil_score,
        'calinski_harabasz_score': ch_score,
        'davies_bouldin_score': db_score,
        'average_degree': avg_degree,
        'num_components': num_components,
        'largest_component_size': largest_comp
    })

df_dbscan = pd.DataFrame(dbscan_metrics)
df_dbscan.sort_values('threshold', inplace=True)
df_dbscan.to_csv('dbscan_metrics_summary.csv', index=False)

if not df_dbscan.empty:
    thresholds = df_dbscan['threshold']
    # Silhouette Score vs Threshold
    plt.figure(figsize=(7,5))
    plt.plot(thresholds, df_dbscan['silhouette_score'], marker='o')
    plt.xlabel('Threshold')
    plt.ylabel('Silhouette Score')
    plt.title('Silhouette Score vs Threshold')
    plt.tight_layout()
    plt.savefig('silhouette_vs_threshold.png')
    plt.close()
    # Calinski-Harabasz Score vs Threshold
    plt.figure(figsize=(7,5))
    plt.plot(thresholds, df_dbscan['calinski_harabasz_score'], marker='o')
    plt.xlabel('Threshold')
    plt.ylabel('Calinski-Harabasz Score')
    plt.title('Calinski-Harabasz Score vs Threshold')
    plt.tight_layout()
    plt.savefig('calinski_harabasz_vs_threshold.png')
    plt.close()
    # Davies-Bouldin Score vs Threshold
    plt.figure(figsize=(7,5))
    plt.plot(thresholds, df_dbscan['davies_bouldin_score'], marker='o')
    plt.xlabel('Threshold')
    plt.ylabel('Davies-Bouldin Score')
    plt.title('Davies-Bouldin Score vs Threshold')
    plt.tight_layout()
    plt.savefig('davies_bouldin_vs_threshold.png')
    plt.close()
    # Average Degree vs Threshold
    plt.figure(figsize=(7,5))
    plt.plot(thresholds, df_dbscan['average_degree'], marker='o')
    plt.xlabel('Threshold')
    plt.ylabel('Average Degree')
    plt.title('Average Degree vs Threshold')
    plt.tight_layout()
    plt.savefig('average_degree_vs_threshold.png')
    plt.close()
    # Number of Components vs Threshold
    plt.figure(figsize=(7,5))
    plt.plot(thresholds, df_dbscan['num_components'], marker='o')
    plt.xlabel('Threshold')
    plt.ylabel('Number of Components')
    plt.title('Number of Components vs Threshold')
    plt.tight_layout()
    plt.savefig('num_components_vs_threshold.png')
    plt.close()
    # Largest Component Size vs Threshold
    plt.figure(figsize=(7,5))
    plt.plot(thresholds, df_dbscan['largest_component_size'], marker='o')
    plt.xlabel('Threshold')
    plt.ylabel('Largest Component Size')
    plt.title('Largest Component Size vs Threshold')
    plt.tight_layout()
    plt.savefig('largest_component_vs_threshold.png')
    plt.close()

# Part 2: KMeans Cluster Sweep
enriched_file = '/content/enriched_data.xlsx'
df_enriched = pd.read_excel(enriched_file)
if 'Enriched Description' in df_enriched.columns:
    descs = df_enriched['Enriched Description'].dropna().astype(str)
    descs = descs[descs.str.strip() != '']
else:
    descs = pd.Series(dtype=str)
if len(descs) > 0:
    vectorizer_k = TfidfVectorizer()
    X_k = vectorizer_k.fit_transform(descs.tolist())
else:
    X_k = None

kmeans_metrics = []
if X_k is not None and X_k.shape[0] > 0:
    X_k_arr = X_k.toarray()
    for k in tqdm(range(5, 51, 5), desc="KMeans clusters"):
        if k < 2 or k > X_k.shape[0]:
            continue
        km = KMeans(n_clusters=k, random_state=42, n_init=10)
        labels_k = km.fit_predict(X_k)
        if len(set(labels_k)) < 2:
            sil_k = np.nan
            ch_k = np.nan
            db_k = np.nan
        else:
            sil_k = silhouette_score(X_k_arr, labels_k)
            ch_k = calinski_harabasz_score(X_k_arr, labels_k)
            db_k = davies_bouldin_score(X_k_arr, labels_k)
        kmeans_metrics.append({
            'num_clusters': k,
            'silhouette_score': sil_k,
            'calinski_harabasz_score': ch_k,
            'davies_bouldin_score': db_k
        })

df_kmeans = pd.DataFrame(kmeans_metrics)
df_kmeans.sort_values('num_clusters', inplace=True)
df_kmeans.to_csv('kmeans_metrics_summary.csv', index=False)

if not df_kmeans.empty:
    ks = df_kmeans['num_clusters']
    plt.figure(figsize=(7,5))
    plt.plot(ks, df_kmeans['silhouette_score'], marker='o')
    plt.xlabel('Number of Clusters')
    plt.ylabel('Silhouette Score')
    plt.title('Silhouette Score vs Number of Clusters')
    plt.tight_layout()
    plt.savefig('silhouette_vs_clusters.png')
    plt.close()
    plt.figure(figsize=(7,5))
    plt.plot(ks, df_kmeans['calinski_harabasz_score'], marker='o')
    plt.xlabel('Number of Clusters')
    plt.ylabel('Calinski-Harabasz Score')
    plt.title('Calinski-Harabasz Score vs Number of Clusters')
    plt.tight_layout()
    plt.savefig('calinski_harabasz_vs_clusters.png')
    plt.close()
    plt.figure(figsize=(7,5))
    plt.plot(ks, df_kmeans['davies_bouldin_score'], marker='o')
    plt.xlabel('Number of Clusters')
    plt.ylabel('Davies-Bouldin Score')
    plt.title('Davies-Bouldin Score vs Number of Clusters')
    plt.tight_layout()
    plt.savefig('davies_bouldin_vs_clusters.png')
    plt.close()

import pandas as pd

# DBSCAN
db = pd.read_csv('dbscan_metrics_summary.csv')
print("DBSCAN – top silhouette:\n",
      db.sort_values('silhouette_score', ascending=False).head(3), "\n")
print("DBSCAN – lowest Davies–Bouldin:\n",
      db.sort_values('davies_bouldin_score').head(3), "\n")

# KMeans
km = pd.read_csv('kmeans_metrics_summary.csv')
print("KMeans – top silhouette:\n",
      km.sort_values('silhouette_score', ascending=False).head(3), "\n")
print("KMeans – lowest Davies–Bouldin:\n",
      km.sort_values('davies_bouldin_score').head(3), "\n")

# ----------------------------- Setup -----------------------------
import os
import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import DBSCAN
from sklearn.neighbors import NearestNeighbors
import networkx as nx

# ------------------------ Load Data -----------------------------
# Enriched descriptions for all SKUs
IN_PATH = '/content/enriched_data.xlsx'
df = pd.read_excel(IN_PATH)
df['Enriched Description'] = df['Enriched Description'].fillna('').astype(str)

# ------------------- TF-IDF Vectorization -----------------------
vect = TfidfVectorizer(max_features=1000, stop_words='english')
X = vect.fit_transform(df['Enriched Description'])

# ------------------ MAIN-CATEGORY CLUSTERING --------------------
# We picked threshold = 0.80 ⇒ eps = 1 − thr = 0.20 for DBSCAN (cosine distance)
eps_main = 1.0 - 0.80
db_main = DBSCAN(eps=eps_main, metric='cosine', min_samples=2, n_jobs=-1)
labels_main = db_main.fit_predict(X)
df['Main Cluster'] = labels_main
df['Main Category'] = df['Main Cluster'].map(
    lambda L: f"Category_{L}" if L >= 0 else "Other"
)

# ------------------ GRAPH METRICS (OPTIONAL) --------------------
# build similarity graph at thr=0.80 to inspect connectivity
nbrs = NearestNeighbors(radius=eps_main, metric='cosine', n_jobs=-1).fit(X)
A = nbrs.radius_neighbors_graph(X, mode='connectivity')
deg = A.sum(axis=1).A1
# e.g., avg_degree = deg.mean()

# ----------------- SUB-CATEGORY CLUSTERING ---------------------
# Within each main cluster, we can tighten to thr=0.90 (eps=0.10)
eps_sub = 1.0 - 0.90
db_sub = DBSCAN(eps=eps_sub, metric='cosine', min_samples=2)
df['Sub-category'] = 'Other'
for L, idxs in df[df['Main Cluster'] >= 0].groupby('Main Cluster').groups.items():
    Xg = X[list(idxs)]
    if Xg.shape[0] >= 3:
        sub_labels = db_sub.fit_predict(Xg)
        for i, lbl in zip(idxs, sub_labels):
            df.at[i, 'Sub-category'] = f"Sub_{L}_{lbl}" if lbl >= 0 else "Other"

# ------------------------- Output ------------------------------
OUT_PATH = '/content/final_categories.xlsx'
df.to_excel(OUT_PATH, index=False)
print(f"Saved Main/Sub categories to {OUT_PATH}")

import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt
import seaborn as sns

# Set plotting style
sns.set(style="whitegrid")

# 1. Load the final categories
df = pd.read_excel('/content/final_categories.xlsx')

# 2. Compute and save category size distributions
main_counts = df['Main Category'].value_counts()
sub_counts = df['Sub-category'].value_counts()

main_counts.to_csv('main_category_counts.csv', header=['count'])
sub_counts.to_csv('sub_category_counts.csv', header=['count'])

# 3. Plot Main Category distribution
plt.figure(figsize=(10, 6))
sns.barplot(x=main_counts.index, y=main_counts.values)
plt.xticks(rotation=90)
plt.xlabel('Main Category')
plt.ylabel('Number of SKUs')
plt.title('Main Category Distribution')
plt.tight_layout()
plt.savefig('main_category_distribution.png')
plt.close()

# 4. Plot Top 20 Sub-category distribution
top20_sub = sub_counts.head(20)
plt.figure(figsize=(10, 6))
sns.barplot(x=top20_sub.index, y=top20_sub.values)
plt.xticks(rotation=90)
plt.xlabel('Sub-category')
plt.ylabel('Number of SKUs')
plt.title('Top 20 Sub-category Distribution')
plt.tight_layout()
plt.savefig('sub_category_top20_distribution.png')
plt.close()

# 5. Compute Silhouette scores for main and sub clusters
#   a) Vectorize descriptions
descs = df['Enriched Description'].fillna('').astype(str)
vect = TfidfVectorizer(max_features=1000, stop_words='english')
X = vect.fit_transform(descs)

#   b) Main clusters
labels_main = df['Main Cluster'].values
mask_main = labels_main >= 0  # exclude noise if any
sil_main = silhouette_score(X[mask_main], labels_main[mask_main], metric='cosine')

#   c) Sub clusters (factorize labels)
labels_sub, _ = pd.factorize(df['Sub-category'])
mask_sub = labels_sub >= 0
sil_sub = silhouette_score(X[mask_sub], labels_sub[mask_sub], metric='cosine')

print(f"Silhouette Score (Main Clusters): {sil_main:.3f}")
print(f"Silhouette Score (Sub Clusters): {sil_sub:.3f}")

# 6. Summary output
print("\nMain Category Counts:")
print(main_counts)
print("\nSub-category Top 20 Counts:")
print(top20_sub)

import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt
import seaborn as sns

# Set plotting style
sns.set(style="whitegrid")

# 1. Load the final categories
df = pd.read_excel('/content/final_categories.xlsx')

# 2. Compute and save category size distributions
main_counts = df['Main Category'].value_counts()
sub_counts = df['Sub-category'].value_counts()

main_counts.to_csv('main_category_counts.csv', header=['count'])
sub_counts.to_csv('sub_category_counts.csv', header=['count'])

# 3. Plot Main Category distribution
plt.figure(figsize=(10, 6))
sns.barplot(x=main_counts.index, y=main_counts.values)
plt.xticks(rotation=90)
plt.xlabel('Main Category')
plt.ylabel('Number of SKUs')
plt.title('Main Category Distribution')
plt.tight_layout()
plt.savefig('main_category_distribution.png')
plt.close()

# 4. Plot Top 20 Sub-category distribution
top20_sub = sub_counts.head(20)
plt.figure(figsize=(10, 6))
sns.barplot(x=top20_sub.index, y=top20_sub.values)
plt.xticks(rotation=90)
plt.xlabel('Sub-category')
plt.ylabel('Number of SKUs')
plt.title('Top 20 Sub-category Distribution')
plt.tight_layout()
plt.savefig('sub_category_top20_distribution.png')
plt.close()

# 5. Compute Silhouette scores for main and sub clusters
#   a) Vectorize descriptions
descs = df['Enriched Description'].fillna('').astype(str)
vect = TfidfVectorizer(max_features=1000, stop_words='english')
X = vect.fit_transform(descs)

#   b) Main clusters
labels_main = df['Main Cluster'].values
mask_main = labels_main >= 0  # exclude noise if any
sil_main = silhouette_score(X[mask_main], labels_main[mask_main], metric='cosine')

#   c) Sub clusters (factorize labels)
labels_sub, _ = pd.factorize(df['Sub-category'])
mask_sub = labels_sub >= 0
sil_sub = silhouette_score(X[mask_sub], labels_sub[mask_sub], metric='cosine')

print(f"Silhouette Score (Main Clusters): {sil_main:.3f}")
print(f"Silhouette Score (Sub Clusters): {sil_sub:.3f}")

# 6. Summary output
print("\nMain Category Counts:")
print(main_counts)
print("\nSub-category Top 20 Counts:")
print(top20_sub)